# ğŸš€ GuÃ­a Completa â€” AWS Data Lake Python Control

Este documento describe cÃ³mo operar, monitorear y extender el pipeline event-driven de ingesta y procesamiento de datos en AWS usando solo Python y VS Code.

## âœ¨ CaracterÃ­sticas principales
- ğŸ”„ Pipeline automÃ¡tico de procesamiento de datos
- ğŸ“Š Dashboard web en tiempo real con Streamlit
- ğŸ” Monitor de sistema integrado
- ğŸ“ˆ MÃ©tricas y visualizaciones
- ğŸš¨ Alertas y notificaciones
- ğŸ› ï¸ Herramientas de testing y debugging

---

## 1. Requisitos previos
- Python 3.8+
- AWS CLI configurado (`aws configure`)
- Acceso a los buckets S3 y la cola SQS definidos en `config/settings.yaml`
- Instalar dependencias:

```bash
python -m venv .venv
.venv/Scripts/activate  # En Windows
pip install -r requirements.txt
```

---

## 2. ConfiguraciÃ³n y VerificaciÃ³n

### Configurar el sistema
- Edita `config/settings.yaml` con los nombres de tus buckets, prefijos y cola SQS.
- AsegÃºrate de tener el perfil de AWS correcto configurado.

### âœ… Verificar que todo funciona (Â¡NUEVO!)

Antes de usar el sistema, ejecuta el script de diagnÃ³stico:

```bash
python test_app.py
```

Este script verifica:
- âœ… ConfiguraciÃ³n cargada correctamente
- âœ… ConexiÃ³n AWS exitosa
- âœ… Glue Catalog funcionando
- âœ… Operaciones S3 funcionando

Si todas las pruebas pasan, el sistema estÃ¡ listo para usar.

---

## ğŸ“Š Dashboard Web Avanzado

Lanza el dashboard interactivo para monitorear tu data lake en tiempo real:

```bash
python main.py dashboard
```

### ğŸ¯ **CaracterÃ­sticas del Dashboard:**

#### **MÃ©tricas Principales (6 cards):**
- ğŸ“ **Archivos RAW** - Archivos sin procesar + tamaÃ±o total
- âœ… **Procesados** - Archivos convertidos a Parquet + tamaÃ±o
- ğŸ“Š **Total Datos** - TamaÃ±o acumulado de todo el sistema
- ğŸ”¥ **Hoy** - Archivos procesados en el dÃ­a actual
- ğŸ“¬ **Cola SQS** - Mensajes pendientes de procesar
- ğŸ”´ **Errores (24h)** - Errores recientes del sistema

#### **AnÃ¡lisis Avanzado (3 grÃ¡ficas):**
- ğŸ“ˆ **Archivos por Tipo** - DistribuciÃ³n de CSV, JSON, Parquet, etc.
- ğŸ° **DistribuciÃ³n por Carpetas** - RAW, Procesados, Athena-results
- ğŸ“Š **Eficiencia del Sistema** - % de archivos procesados con barra de progreso

#### **Estado del Sistema:**
- ğŸš¦ **Estado General** - Operativo/Problemas/AtenciÃ³n
- ğŸ¤– **Worker Status** - Detecta si el worker estÃ¡ corriendo (PID + tiempo activo)

#### **Lector de Archivos Integrado (Â¡NUEVO!):**
- ğŸ“– **SelecciÃ³n interactiva** - Elige cualquier archivo de la lista numerada
- ğŸ“ **Todos los tipos** - Parquet, JSON, JSONL, CSV, TXT, Metadata
- â¬‡ï¸ **Descarga directa** - BotÃ³n para descargar archivos completos
- ğŸ“ˆ **AnÃ¡lisis completo** - 5 pestaÃ±as de exploraciÃ³n de datos:
  1. **ğŸ” Vista Previa** - Primeras/Ãºltimas/aleatorias filas
  2. **ğŸ“Š EstadÃ­sticas** - DescripciÃ³n completa y anÃ¡lisis de nulos
  3. **ğŸ·ï¸ Tipos de Datos** - AnÃ¡lisis detallado por columna
  4. **ğŸ“ˆ GrÃ¡ficas** - Histogramas, correlaciones, box plots
  5. **ğŸ” Explorar** - Filtros dinÃ¡micos y bÃºsqueda de texto
- ğŸ“¥ **Exportar datos** - CSV para Excel/Google Sheets
- ğŸ” **Filtros avanzados** - Por columna y bÃºsqueda de texto

#### **Explorador de Archivos Avanzado:**
- ğŸ” **Filtros mÃºltiples:**
  - **Origen:** Procesados / RAW / Todos los buckets
  - **Tipo:** Todos / parquet / jsonl / csv / json / txt / metadata
  - **Fecha:** Filtro opcional por dÃ­a especÃ­fico
- ğŸ“„ **Tabla optimizada:**
  - NumeraciÃ³n automÃ¡tica (#)
  - Columnas: Archivo, Tipo, TamaÃ±o, Fecha
  - PaginaciÃ³n inteligente (20 archivos por pÃ¡gina)
  - Selector de pÃ¡gina en esquina inferior derecha

### ğŸ® **CÃ³mo usar el Dashboard:**

1. **Monitoreo general:** Las 6 mÃ©tricas te dan una vista rÃ¡pida del sistema
2. **AnÃ¡lisis detallado:** Las 3 grÃ¡ficas muestran distribuciones y eficiencia
3. **Verificar worker:** La secciÃ³n Worker Status te dice si estÃ¡ corriendo
4. **Explorar archivos:** Usa los filtros para encontrar archivos especÃ­ficos
5. **Leer archivos:** Selecciona cualquier archivo y usa "Leer Archivo" para anÃ¡lisis completo
6. **Descargar archivos:** BotÃ³n "Descargar" para cualquier tipo de archivo
7. **Exportar datos:** Usa "Exportar CSV" para abrir en Excel/Google Sheets
8. **NavegaciÃ³n:** Usa el selector de pÃ¡gina para ver mÃ¡s archivos

### âš™ï¸ **ConfiguraciÃ³n:**
- **Auto-refresh:** Desactivado por defecto (activa manualmente si quieres)
- **Cache:** 30 segundos para mejor rendimiento
- **Acceso:** http://localhost:8501

---

## ğŸ” Monitor del Sistema (Â¡NUEVO!)

Ejecuta el monitor para obtener mÃ©tricas detalladas:

```powershell
$env:PYTHONPATH="src"; & ".venv/Scripts/python.exe" "scripts/run_monitor.py"
```

El monitor te muestra:
- Archivos en buckets S3 (RAW y procesados)
- Mensajes en cola SQS
- Logs de errores recientes
- MÃ©tricas de rendimiento

---

## 3. Comandos principales

Usa `main.py` para ejecutar todas las operaciones:

### Worker automÃ¡tico (procesamiento continuo)
```bash
python main.py worker
```
Se queda corriendo, procesando mensajes de SQS automÃ¡ticamente.

### Dashboard web interactivo
```bash
python main.py dashboard
```
Abre el dashboard en http://localhost:8501

### Probar pipeline end-to-end
```bash
python main.py pipeline
```
Sube archivo de prueba y activa el flujo completo.

### Actualizar catÃ¡logo de Glue
```bash
python main.py glue
```
Registra archivos Parquet como tablas SQL.

### Consultar con Athena
```bash
python main.py athena
```
Ejecuta consulta SQL de ejemplo sobre los datos.

### ğŸ” Athena Interactivo (Â¡NUEVO!)
```bash
python main.py athena-sql
```
Consola SQL interactiva para ejecutar consultas personalizadas en tiempo real.

#### **CaracterÃ­sticas del Athena Interactivo:**
- **Consultas multilÃ­nea** - Escribe SQL complejo en mÃºltiples lÃ­neas
- **Comandos especiales:**
  - `tables` - Ver todas las tablas disponibles
  - `schema TABLA` - Ver columnas y tipos de una tabla
  - `exit` - Salir del programa
- **Resultados en tiempo real** - Ve los datos inmediatamente
- **Formato tabla** - Resultados organizados y fÃ¡ciles de leer

#### **CÃ³mo usar Athena Interactivo:**

1. **Ejecutar comando:**
   ```bash
   python main.py athena-sql
   ```

2. **Ver tablas disponibles:**
   ```sql
   ğŸ” SQL> tables
   ğŸ“‹ TABLAS DISPONIBLES:
     â€¢ year_2025 (2 columnas) - s3://bucket/processed/events/year=2025/
     â€¢ year_2026 (3 columnas) - s3://bucket/processed/events/year=2026/
   ```

3. **Ver esquema de tabla:**
   ```sql
   ğŸ” SQL> schema year_2026
   ğŸ“Š ESQUEMA DE TABLA: year_2026
   --------------------------------------------------
     event_time           | bigint
     user_id              | bigint
     action               | string
   ```

4. **Consultas de una lÃ­nea:**
   ```sql
   ğŸ” SQL> SELECT COUNT(*) FROM year_2026;
   âœ… Consulta exitosa! (2 filas)
   ğŸ“Š RESULTADOS:
   ----------------
   _col0
   ----------------
   16
   ```

5. **Consultas multilÃ­nea:**
   ```sql
   ğŸ” SQL> SELECT action, COUNT(*) as cantidad
        ... FROM year_2026 
        ... GROUP BY action
        ... ORDER BY cantidad DESC;
   âœ… Consulta exitosa! (3 filas)
   ğŸ“Š RESULTADOS:
   ----------------
   action    | cantidad
   ----------------
   login     | 8
   logout    | 5
   view      | 3
   ```

#### **Ejemplos de consultas Ãºtiles:**
```sql
-- Ver todos los datos
SELECT * FROM year_2026 LIMIT 10;

-- AnÃ¡lisis por usuario
SELECT user_id, COUNT(*) as eventos
FROM year_2026 
GROUP BY user_id 
ORDER BY eventos DESC;

-- Convertir timestamp a fecha legible
SELECT 
    FROM_UNIXTIME(event_time/1000000000) as fecha,
    user_id,
    action
FROM year_2026 
ORDER BY event_time DESC 
LIMIT 5;

-- Actividad por tipo de acciÃ³n
SELECT action, COUNT(*) as total
FROM year_2026 
GROUP BY action;
```

### Leer archivos desde terminal (Â¡NUEVO!)
```bash
python main.py read
```
Lector interactivo de archivos con anÃ¡lisis completo de datos.

### Sincronizar con S3
```bash
# Ver todos los archivos
python main.py s3-sync --bucket tu-bucket-name

# Ver archivos de una carpeta especÃ­fica
python main.py s3-sync --bucket tu-bucket --prefix processed/

# Limitar nÃºmero de archivos mostrados
python main.py s3-sync --bucket tu-bucket --prefix raw/ --limit 5

# Ver archivos de una fecha especÃ­fica
python main.py s3-sync --bucket tu-bucket --prefix raw/ --date 2026-01-08

# Ver los Ãºltimos N archivos subidos (mÃ¡s recientes)
python main.py s3-sync --bucket tu-bucket --latest 3

# Combinar filtros: Ãºltimos 5 archivos de hoy
python main.py s3-sync --bucket tu-bucket --latest 5 --date 2026-01-08
```
Explora archivos en S3 con filtros avanzados por fecha y cantidad.

---

## ğŸ“– Lector de Archivos Interactivo (Â¡NUEVO!)

Lee y analiza cualquier archivo de tu data lake directamente desde la terminal:

```bash
python main.py read
```

### ğŸ¯ **CaracterÃ­sticas del Lector:**

#### **ğŸ“ SelecciÃ³n de archivos:**
- **Lista completa** - Ve todos los archivos disponibles (RAW + Procesados)
- **Tabla organizada** - NÃºmero, nombre, tipo, tamaÃ±o, origen, ruta completa
- **SelecciÃ³n interactiva** - Escribe el nÃºmero o presiona ENTER para el mÃ¡s reciente
- **Tipos soportados** - Parquet, JSON, JSONL, CSV, TXT, Metadata

#### **ğŸ“Š AnÃ¡lisis completo de datos:**
- **InformaciÃ³n bÃ¡sica** - Dimensiones, memoria, columnas
- **Vista previa** - Primeras 5 filas con formato limpio
- **EstadÃ­sticas numÃ©ricas** - DescripciÃ³n completa (mean, std, min, max, etc.)
- **InformaciÃ³n de columnas** - Tipos, nulos, valores Ãºnicos
- **Valores categÃ³ricos** - Top 5 valores mÃ¡s frecuentes

#### **ğŸ“„ Archivos de texto y metadata:**
- **MÃºltiples codificaciones** - UTF-8, Latin-1, ASCII, CP1252
- **Archivos binarios** - Muestra contenido hexadecimal si no es texto
- **Metadata de Athena** - Lee archivos .metadata con informaciÃ³n de consultas
- **Truncamiento inteligente** - Limita contenido largo para mejor legibilidad

### ğŸš€ **CÃ³mo usar:**

1. **Ejecutar comando:**
   ```bash
   python main.py read
   ```

2. **Ver lista de archivos:**
   ```
   ğŸ“ ARCHIVOS DISPONIBLES (40):
   #    Archivo                             Tipo       TamaÃ±o     Origen          Ruta Completa
   1    test_20260108_002247.jsonl          JSONL      159B       RAW-Todos       raw/events/incoming/test_20260108_002247.jsonl
   2    metadata.csv                        CSV        1.2KB      RAW-Todos       athena-results/metadata.csv
   3    test.parquet                        PARQUET    2.3KB      Procesados      processed/events/test.parquet
   ```

3. **Seleccionar archivo:**
   ```
   ğŸ¯ Selecciona archivo (1-40) o ENTER para el mÃ¡s reciente: 3
   ğŸ“– Seleccionado: test.parquet
   ```

4. **Ver anÃ¡lisis completo:**
   ```
   ğŸ“Š RESUMEN DEL ARCHIVO
   ğŸ“ Archivo: test.parquet
   ğŸ“Œ Ruta: processed/events/test.parquet
   ğŸ“Š Dimensiones: 1,234 filas Ã— 5 columnas
   ğŸ’¾ Memoria: 45.2 KB
   ğŸ“‹ Columnas: event_time, user_id, action, value, category
   
   ğŸ” VISTA PREVIA (primeras 5 filas)
   [tabla con datos]
   
   ğŸ“ˆ ESTADÃSTICAS NUMÃ‰RICAS
   [estadÃ­sticas detalladas]
   
   ğŸ·ï¸ INFORMACIÃ“N DE COLUMNAS
   [tipos, nulos, Ãºnicos por columna]
   ```

### ğŸ’¡ **Ventajas:**
- **Sin configuraciÃ³n** - Funciona inmediatamente
- **Todos los archivos** - Ve archivos de cualquier carpeta (RAW, procesados, athena-results)
- **AnÃ¡lisis instantÃ¡neo** - EstadÃ­sticas completas sin escribir cÃ³digo
- **Interfaz limpia** - Salida organizada y fÃ¡cil de leer
- **Manejo robusto** - Soporta diferentes codificaciones y archivos binarios

---

## 4. ğŸƒ Inicio RÃ¡pido - Flujo completo

### Paso 1: Verificar sistema
```bash
python test_app.py
```

### Paso 2: Ejecutar worker (Terminal 1)
```bash
python main.py worker
```

### Paso 3: Dashboard (Terminal 2)
```bash
python main.py dashboard
```

### Paso 4: Probar pipeline (Terminal 3)
```bash
python main.py pipeline
```

Â¡VerÃ¡s los archivos procesÃ¡ndose en tiempo real! ğŸš€

---

## 5. Probar el pipeline end-to-end

Genera y sube un archivo de prueba, y envÃ­a el mensaje a SQS:

```bash
python main.py pipeline
```

Esto simula la llegada de un archivo nuevo y activa el flujo completo.

---

## 6. ğŸ“ Explorar archivos S3 (avanzado)

El comando `s3-sync` tiene filtros potentes para encontrar exactamente lo que necesitas:

```bash
# "MuÃ©strame los Ãºltimos 3 archivos que se subieron"
python main.py s3-sync --bucket tu-bucket --latest 3

# "Â¿QuÃ© archivos llegaron el 8 de enero?"
python main.py s3-sync --bucket tu-bucket --date 2026-01-08

# "Los Ãºltimos 5 archivos de hoy en la carpeta RAW"
python main.py s3-sync --bucket tu-bucket --prefix raw/ --latest 5 --date 2026-01-08

# "Solo muÃ©strame 10 archivos de la carpeta procesados"
python main.py s3-sync --bucket tu-bucket --prefix processed/ --limit 10
```

### **ParÃ¡metros disponibles:**
- `--prefix carpeta/` - Filtrar por carpeta
- `--date YYYY-MM-DD` - Filtrar por fecha especÃ­fica
- `--latest N` - Los N archivos mÃ¡s recientes
- `--limit N` - MÃ¡ximo N archivos a mostrar

### **InformaciÃ³n mostrada:**
- âœ… Ruta completa del archivo
- âœ… Fecha y hora de subida
- âœ… Ordenado por mÃ¡s recientes
- âœ… Contador total de archivos


---

## 7. Leer archivos Parquet desde Python (opcional)

Instala pandas y pyarrow si no los tienes:

```bash
pip install pandas pyarrow
```

Ejemplo de lectura:

```python
import pandas as pd
import s3fs

bucket = "<tu-bucket>"
key = "<ruta-al-archivo.parquet>"

s3_path = f"s3://{bucket}/{key}"
df = pd.read_parquet(s3_path, storage_options={"anon": False})
print(df)
```

---

## 8. Consultar datos con Athena desde Python

Puedes lanzar consultas SQL sobre tus datos procesados en S3 usando Athena y obtener los resultados directamente en Python.

Ejecuta el script de ejemplo:

```powershell
$env:PYTHONPATH="src"; & ".venv/Scripts/python.exe" "scripts/athena_query_example.py"
```

El script lanza una consulta como:

```sql
SELECT * FROM datalake_processed_db.year_2026 LIMIT 10;
```

Y muestra los resultados en la terminal. Puedes modificar la consulta y la tabla segÃºn tus necesidades.

Recuerda que Athena necesita un bucket de resultados (output_location) con permisos de escritura.

---

---

## 9. ğŸ› ï¸ Herramientas de Debugging

### Verificar sistema completo
```bash
python test_app.py
```

### Comandos principales
```bash
# Worker automÃ¡tico
python main.py worker

# Dashboard web
python main.py dashboard

# Probar pipeline
python main.py pipeline

# Actualizar catÃ¡logo
python main.py glue

# Consultar con Athena
python main.py athena

# Athena Interactivo (Â¡NUEVO!)
python main.py athena-sql

# Leer archivos desde terminal
python main.py read

# Ver archivos S3 (bÃ¡sico)
python main.py s3-sync --bucket tu-bucket

# Ver archivos S3 (avanzado)
python main.py s3-sync --bucket tu-bucket --latest 5 --date 2026-01-08
```

### Monitor puntual (mÃ©todo anterior)
```powershell
$env:PYTHONPATH="src"; & ".venv/Scripts/python.exe" "scripts/run_monitor.py"
```

### Limpiar cola SQS (si es necesario)
```python
import boto3
sqs = boto3.client('sqs')
sqs.purge_queue(QueueUrl='tu-queue-url')
```

---

## 10. Tips y troubleshooting
- ğŸ“Š **Dashboard lento**: Desactiva auto-refresh si tienes muchos archivos
- ğŸ”„ **Worker no procesa**: Revisa credenciales, permisos y formato del mensaje SQS
- ğŸ“„ **Logs**: Monitorea `logs/worker.log` para errores detallados
- ğŸ§¹ **Limpiar cola**: Usa la consola AWS o boto3 para purgar mensajes
- ğŸ“Š **MÃ©tricas**: El dashboard guarda cache por 30s para mejor rendimiento

---

## 11. ğŸš€ Extensiones y mejoras sugeridas
### âœ… COMPLETAMENTE IMPLEMENTADO
- âœ… Dashboard web interactivo con 6 mÃ©tricas en tiempo real
- âœ… AnÃ¡lisis avanzado con 3 grÃ¡ficas interactivas
- âœ… Explorador de archivos con filtros mÃºltiples (tipo, fecha, origen)
- âœ… Lector de archivos integrado en dashboard (5 pestaÃ±as completas)
- âœ… Lector de archivos interactivo por terminal
- âœ… Descarga de archivos desde dashboard
- âœ… Soporte completo para todos los tipos de archivo (parquet, json, csv, txt, metadata)
- âœ… AnÃ¡lisis completo de datos con estadÃ­sticas y grÃ¡ficas
- âœ… Manejo robusto de codificaciones y archivos binarios
- âœ… PaginaciÃ³n inteligente y numeraciÃ³n
- âœ… Worker status en tiempo real con detecciÃ³n de PID
- âœ… Monitor de sistema integrado
- âœ… Comandos CLI simplificados y unificados
- âœ… Filtros S3 avanzados por fecha y tipo
- âœ… **Athena Interactivo** - Console SQL con consultas multilÃ­nea
- âœ… **Comandos especiales** - `tables`, `schema`, navegaciÃ³n intuitiva
- âœ… **AnÃ¡lisis en tiempo real** - Resultados SQL inmediatos

### ğŸš§ PrÃ³ximas mejoras sugeridas
- ValidaciÃ³n de esquema de datos automÃ¡tica
- Manejo de errores avanzado y Dead Letter Queue (DLQ)
- Notificaciones por email/Slack cuando hay errores
- AutomatizaciÃ³n completa de Glue/Athena
- Historial de mÃ©tricas en base de datos
- Tests automatizados y CI/CD
- Alertas proactivas de rendimiento

---

**Â¡Con esta guÃ­a puedes operar, monitorear y probar todo el pipeline sin depender de la consola web de AWS!** ğŸ‰

### ğŸ“± Accesos rÃ¡pidos
- **Dashboard**: `python main.py dashboard` â†’ http://localhost:8501
- **Lector de archivos**: `python main.py read`
- **Worker**: `python main.py worker`
- **Pipeline**: `python main.py pipeline`
- **Glue**: `python main.py glue`
- **Athena (ejemplo)**: `python main.py athena`
- **ğŸ” Athena Interactivo**: `python main.py athena-sql`
- **S3 (bÃ¡sico)**: `python main.py s3-sync --bucket tu-bucket`
- **S3 (filtros)**: `python main.py s3-sync --bucket tu-bucket --latest 3`
- **Verificar**: `python test_app.py`
- **Logs**: `logs/worker.log`
- **Config**: `config/settings.yaml`
