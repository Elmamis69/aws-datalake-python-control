# ğŸš€ GuÃ­a Completa â€” AWS Data Lake Python Control

Este documento describe cÃ³mo operar, monitorear y extender el pipeline event-driven de ingesta y procesamiento de datos en AWS usando solo Python y VS Code.

## âœ¨ CaracterÃ­sticas principales
- ğŸ”„ Pipeline automÃ¡tico de procesamiento de datos
- ğŸ“Š Dashboard web en tiempo real con Streamlit
- ğŸ” Monitor de sistema integrado
- ğŸ“ˆ MÃ©tricas y visualizaciones
- ğŸš¨ Alertas y notificaciones
- ğŸ› ï¸ Herramientas de testing y debugging

---

## 1. Requisitos previos
- Python 3.8+
- AWS CLI configurado (`aws configure`)
- Acceso a los buckets S3 y la cola SQS definidos en `config/settings.yaml`
- Instalar dependencias:

```bash
python -m venv .venv
.venv/Scripts/activate  # En Windows
pip install -r requirements.txt
```

---

## 2. ConfiguraciÃ³n
- Edita `config/settings.yaml` con los nombres de tus buckets, prefijos y cola SQS.
- AsegÃºrate de tener el perfil de AWS correcto configurado.

---

## ğŸ“Š Dashboard Web (Â¡NUEVO!)

Lanza el dashboard interactivo para monitorear tu data lake en tiempo real:

```bash
streamlit run dashboard/app.py
```

El dashboard incluye:
- ğŸ“ Conteo de archivos RAW y procesados
- ğŸ“¬ Estado de la cola SQS
- ğŸš¨ Errores recientes (Ãºltimas 24h)
- ğŸ“ˆ GrÃ¡ficas de tendencias
- ğŸ”„ Auto-refresh cada 30 segundos
- ğŸš¦ Estado general del sistema

Accede en: http://localhost:8501

---

## ğŸ” Monitor del Sistema (Â¡NUEVO!)

Ejecuta el monitor para obtener mÃ©tricas detalladas:

```powershell
$env:PYTHONPATH="src"; & ".venv/Scripts/python.exe" "scripts/run_monitor.py"
```

El monitor te muestra:
- Archivos en buckets S3 (RAW y procesados)
- Mensajes en cola SQS
- Logs de errores recientes
- MÃ©tricas de rendimiento

---

## 3. Ejecutar el worker (procesador principal)

Desde la raÃ­z del proyecto:

```powershell
$env:PYTHONPATH="src"; & ".venv/Scripts/python.exe" "scripts/run_worker.py"
```

El worker quedarÃ¡ escuchando la cola SQS y procesarÃ¡ archivos automÃ¡ticamente.

ğŸ’¡ **Tip**: MantÃ©n el worker corriendo en una terminal mientras usas el dashboard en otra.

---

## 4. ğŸƒ Inicio RÃ¡pido - Todo en uno

Para probar todo el sistema de una vez:

1. **Terminal 1** - Worker:
```powershell
$env:PYTHONPATH="src"; & ".venv/Scripts/python.exe" "scripts/run_worker.py"
```

2. **Terminal 2** - Dashboard:
```bash
streamlit run dashboard/app.py
```

3. **Terminal 3** - Test:
```powershell
$env:PYTHONPATH="src"; & ".venv/Scripts/python.exe" "scripts/test_pipeline.py"
```

Â¡VerÃ¡s los archivos procesÃ¡ndose en tiempo real en el dashboard! ğŸš€

---

## 5. Probar el pipeline end-to-end

Genera y sube un archivo de prueba, y envÃ­a el mensaje a SQS:

```powershell
$env:PYTHONPATH="src"; & ".venv/Scripts/python.exe" "scripts/test_pipeline.py"
```

Esto simula la llegada de un archivo nuevo y activa el flujo completo.

---

## 6. Listar archivos procesados en S3 desde Python

Para ver los archivos Parquet generados en la ruta de salida:

```powershell
$env:PYTHONPATH="src"; & ".venv/Scripts/python.exe" "scripts/list_s3_processed.py"
```

Puedes modificar la fecha en el script para buscar otros dÃ­as.

---

## 7. Leer archivos Parquet desde Python (opcional)

Instala pandas y pyarrow si no los tienes:

```bash
pip install pandas pyarrow
```

Ejemplo de lectura:

```python
import pandas as pd
import s3fs

bucket = "<tu-bucket>"
key = "<ruta-al-archivo.parquet>"

s3_path = f"s3://{bucket}/{key}"
df = pd.read_parquet(s3_path, storage_options={"anon": False})
print(df)
```

---

## 8. Consultar datos con Athena desde Python

Puedes lanzar consultas SQL sobre tus datos procesados en S3 usando Athena y obtener los resultados directamente en Python.

Ejecuta el script de ejemplo:

```powershell
$env:PYTHONPATH="src"; & ".venv/Scripts/python.exe" "scripts/athena_query_example.py"
```

El script lanza una consulta como:

```sql
SELECT * FROM datalake_processed_db.year_2026 LIMIT 10;
```

Y muestra los resultados en la terminal. Puedes modificar la consulta y la tabla segÃºn tus necesidades.

Recuerda que Athena necesita un bucket de resultados (output_location) con permisos de escritura.

---

---

## 9. ğŸ› ï¸ Herramientas de Debugging

### Monitor puntual
```powershell
$env:PYTHONPATH="src"; & ".venv/Scripts/python.exe" "scripts/run_monitor.py"
```

### Verificar configuraciÃ³n
```powershell
$env:PYTHONPATH="src"; & ".venv/Scripts/python.exe" "scripts/test_config.py"
```

### Limpiar cola SQS (si es necesario)
```python
import boto3
sqs = boto3.client('sqs')
sqs.purge_queue(QueueUrl='tu-queue-url')
```

---

## 10. Tips y troubleshooting
- ğŸ“Š **Dashboard lento**: Desactiva auto-refresh si tienes muchos archivos
- ğŸ”„ **Worker no procesa**: Revisa credenciales, permisos y formato del mensaje SQS
- ğŸ“„ **Logs**: Monitorea `logs/worker.log` para errores detallados
- ğŸ§¹ **Limpiar cola**: Usa la consola AWS o boto3 para purgar mensajes
- ğŸ“Š **MÃ©tricas**: El dashboard guarda cache por 30s para mejor rendimiento

---

## 11. ğŸš€ Extensiones y mejoras sugeridas
### Implementadas âœ…
- âœ… Dashboard web interactivo
- âœ… Monitor de sistema en tiempo real
- âœ… MÃ©tricas y visualizaciones
- âœ… Auto-refresh y alertas visuales

### Por implementar ğŸš§
- ValidaciÃ³n de esquema de datos
- Manejo de errores avanzado y DLQ
- Notificaciones por email/Slack
- AutomatizaciÃ³n de Glue/Athena
- Historial de mÃ©tricas en base de datos
- Tests automatizados

---

**Â¡Con esta guÃ­a puedes operar, monitorear y probar todo el pipeline sin depender de la consola web de AWS!** ğŸ‰

### ğŸ“± Accesos rÃ¡pidos
- **Dashboard**: http://localhost:8501
- **Logs**: `logs/worker.log`
- **Config**: `config/settings.yaml`
- **Tests**: `scripts/test_pipeline.py`
