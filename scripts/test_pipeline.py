"""
Pipeline de Prueba End-to-End

Este script simula el flujo completo del Data Lake:
1. Genera un archivo JSONL temporal con datos de prueba
2. Lo sube al bucket S3 RAW (datos sin procesar)
3. Env√≠a un mensaje a SQS con la ubicaci√≥n del archivo
4. El worker SQS procesar√° autom√°ticamente el archivo
5. Limpia el archivo temporal local

Es √∫til para verificar que toda la infraestructura funciona correctamente.

Uso: python scripts/test_pipeline.py
"""

import boto3
import json
import os
from datetime import datetime, timezone

# Cargar configuraci√≥n desde settings.yaml
import yaml

# Cargar configuraci√≥n del proyecto
CONFIG_PATH = os.path.join(os.path.dirname(__file__), '../config/settings.yaml')
with open(CONFIG_PATH, 'r') as f:
    config = yaml.safe_load(f)

# Extraer configuraci√≥n AWS
aws_conf = config['aws']
s3_raw_bucket = aws_conf['s3_raw_bucket']      # Bucket para datos sin procesar
s3_raw_prefix = aws_conf['s3_raw_prefix']      # Prefijo/carpeta en S3
queue_url = aws_conf['sqs_queue_url']          # Cola SQS para notificaciones
region = aws_conf.get('region', 'us-east-2')  # Regi√≥n AWS

print(f"Configuraci√≥n cargada - Bucket: {s3_raw_bucket}, Cola: {queue_url}")

# PASO 1: Crear archivo JSONL temporal con datos de prueba
print("\nüìÑ Paso 1: Generando archivo de prueba...")
data = [
    {"event_time": datetime.now(timezone.utc).isoformat(), "user_id": 1, "action": "login"},
    {"event_time": datetime.now(timezone.utc).isoformat(), "user_id": 2, "action": "logout"}
]

# Crear archivo JSONL (JSON Lines - un JSON por l√≠nea)
jsonl_path = "test.jsonl"
with open(jsonl_path, 'w') as f:
    for row in data:
        f.write(json.dumps(row) + '\n')
        
print(f"‚úÖ Archivo creado: {jsonl_path} con {len(data)} registros")

# PASO 2: Subir archivo al bucket S3 RAW
print("\nüì§ Paso 2: Subiendo archivo a S3...")
# Generar nombre √∫nico con timestamp para evitar conflictos
s3_key = f"{s3_raw_prefix}test_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}.jsonl"

s3 = boto3.client('s3', region_name=region)
s3.upload_file(jsonl_path, s3_raw_bucket, s3_key)
print(f"‚úÖ Archivo subido a: s3://{s3_raw_bucket}/{s3_key}")

# PASO 3: Notificar al worker v√≠a SQS
print("\nüì® Paso 3: Enviando notificaci√≥n a SQS...")
# El worker SQS procesar√° autom√°ticamente el archivo cuando reciba este mensaje
sqs = boto3.client('sqs', region_name=region)
sqs.send_message(QueueUrl=queue_url, MessageBody=s3_key)
print(f"‚úÖ Mensaje enviado a SQS - El worker procesar√° el archivo autom√°ticamente")
print(f"   Contenido del mensaje: {s3_key}")

# PASO 4: Limpiar archivo temporal local
print("\nüßπ Paso 4: Limpiando archivos temporales...")
os.remove(jsonl_path)
print(f"‚úÖ Archivo temporal eliminado: {jsonl_path}")

print("\nüéâ Pipeline de prueba completado exitosamente!")
print("\nüîç Pr√≥ximos pasos:")
print("   1. El worker SQS procesar√° el archivo autom√°ticamente")
print("   2. Los datos se convertir√°n a formato Parquet")
print("   3. Se actualizar√° el cat√°logo de Glue")
print("   4. Podr√°s consultar los datos en Athena")
print("\nüìä Para monitorear: python main.py dashboard")
